{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try at NLTK analyses of the non-excluded beauty descriptions\n",
    "Useful reseource for basic operations: https://www.nltk.org/book/\n",
    "\n",
    "For other processing steps: https://chrisalbon.com/machine_learning/preprocessing_text/remove_stop_words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('beauty_descriptions_US.txt', 'r')\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize, convert to text, set to lowercase, grab total vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "sentences = nltk.sent_tokenize(raw)\n",
    "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "words = [w.lower() for w in tokens]\n",
    "vocab = sorted(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation = ['.',',','!',';','(',')']\n",
    "clean_text = nltk.Text([word for word in words if word not in stop_words and word not in punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for patterns, keywords, bigrams\n",
    "It seems like there is a pre-built package that extracts key words/phrases from a text. https://pypi.org/project/rake-nltk/. However, I do not find that their extracted ranking makes any sense. It merely seems to capture alphabetical order?\n",
    "\n",
    "I think, we can for now just get away with a frequency count, discounting stop words.\n",
    "\n",
    "Let's look at similar words, and co-occurring words. \n",
    "\n",
    "*Note:* It seems like we do not have enough data for finding similar words once we stripped off stopwords and the like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up there beauty younger i the reds trees out setting not perfect\n",
      "amazing intense mesmerizing original kayak while bright breathtaking\n",
      "beautiful nature top it experience crunch edge being part epitome\n",
      "kayak anything while smells point sound grass company clearness bottom\n"
     ]
    }
   ],
   "source": [
    "# find words that are similar to both beauty and beautiful\n",
    "# logic: what applies to both gets rid of random co-occurrences more so than just the function alone\n",
    "# NOTE that .similar() and .similar_words do produce different outputs, so it is useful to look at both\n",
    "# However, we can only autmatically look at the overlap between outputs for beauty and beautiful using similar_words\n",
    "text.similar('beautiful')\n",
    "text.similar('beauty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I was trying to find the overlapping portion of the similar words only, but that seems to fail and not be very interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_beautiful = text._word_context_index.similar_words(\"beautiful\")\n",
    "sim_beauty = text._word_context_index.similar_words(\"beauty\")\n",
    "#sim_beaut = [word for word in sim_beautiful if word in sim_beauty]\n",
    "#print(sim_beaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['dramatic' 'top']]\n",
      "\n",
      " [['transforming' 'nature']]\n",
      "\n",
      " [['peaceful' 'smell']]\n",
      "\n",
      " [['out' 'experience']]\n",
      "\n",
      " [['there' 'it']]\n",
      "\n",
      " [['younger' 'freedom']]\n",
      "\n",
      " [['up' 'perfume']]\n",
      "\n",
      " [['breathtaking' 'anything']]\n",
      "\n",
      " [['perfect' 'magic']]\n",
      "\n",
      " [['loaded' 'grass']]\n",
      "\n",
      " [['incredible' 'beautiful']]\n",
      "\n",
      " [['bright' 'edge']]\n",
      "\n",
      " [['warm' 'sound']]\n",
      "\n",
      " [['going' 'presence']]\n",
      "\n",
      " [['setting' 'birth']]\n",
      "\n",
      " [['blue' 'whole']]\n",
      "\n",
      " [['amazing' 'crunch']]\n",
      "\n",
      " [['beauty' 'part']]\n",
      "\n",
      " [['precious' 'epitome']]\n",
      "\n",
      " [['encased' 'smells']]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.transpose([[sim_beautiful],[sim_beauty]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beautiful', 69),\n",
       " ('experience', 35),\n",
       " ('could', 28),\n",
       " ('like', 27),\n",
       " ('beauty', 27),\n",
       " ('see', 26),\n",
       " ('one', 24),\n",
       " ('time', 22),\n",
       " ('went', 22),\n",
       " ('remember', 22),\n",
       " ('day', 21),\n",
       " ('saw', 20),\n",
       " (\"'s\", 20),\n",
       " ('would', 18),\n",
       " (\"n't\", 17)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "fdist_words = FreqDist(clean_text)\n",
    "fdist_words.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('beauty', 'experience'), 9),\n",
       " (('could', 'see'), 8),\n",
       " (('first', 'time'), 7),\n",
       " (('beautiful', 'experience'), 6),\n",
       " (('sun', 'setting'), 5),\n",
       " (('ever', 'seen'), 5),\n",
       " (('able', 'see'), 5),\n",
       " (('felt', 'like'), 5),\n",
       " (('made', 'feel'), 5),\n",
       " (('top', 'mountain'), 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram freq - use this rather than mere 'collocations' because here we do get a proper frequency count.\n",
    "bigrm = nltk.bigrams(clean_text)\n",
    "fdist_bigrams = FreqDist(bigrm)\n",
    "fdist_bigrams.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use empath package for LIWC like analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'empath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-042cf3ba8016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mempath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'empath'"
     ]
    }
   ],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "\n",
    "lexicon.analyze(raw, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.analyze(raw, categories=[\"social\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.analyze(raw, categories=[\"beauty\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
